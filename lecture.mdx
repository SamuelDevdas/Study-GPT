 Okay, it's recording now. Yeah, before we start couple of things, you don't see anything, I guess. So I share my screen. Do you see my screen? Yes. Good. Yeah, a couple of things on what I uploaded. So I don't know if you have seen it, but there is in the lecture notes, there is a new folder with video recordings online sessions. And yeah, so the uploaded online sessions will be there. What else? I mean, here normally you have key lecture notes as a PDF. There is basically, yeah, another small folder in the scripts with tiny diffusion. We will talk about this today among other things. And then there is a folder flow wise with two chat flows. I will come to that why at the end, we will flow wise and not long flow. Yeah, so just that you are aware that some stuff basically happened on that platform here. So I have a question to you. Did I answer all the emails you said to me? Is there somebody who is still waiting for an answer? So I guess that means no. Good. Are there any questions regarding the last time related to topics we discussed and went through the last time? Teams also not to be the case. So, yeah, so please. When you, I mean, when you don't understand something, when you have some remarks, or even a topic you want to discuss related to what I present, then please interrupt and hook in. Yeah, it would be cool when we have a little more interaction. So it's on the online side, let's say, of these kind of lectures. It's probably a little bit more difficult. But I think next week we'll see us in person. And we will have a hands-on session than anyway. So then there will be much more interaction. The first place. Yeah, okay. So what are we doing today? So we have two topics. Basically, I want to cover with you. One is something we didn't have time to cover the last time about building blocks of generative AI. So what we did was we had a look at difference. Let's say, recurrent neural network, style stuff, mark of chains and transformers. Of course, transformers, they play not only a role when it comes to text generation, but also when it comes to image generation. And one particular, let's say, building block when it comes to image generation, or something like text to image. So when you give a prompt, like a draw, I don't know, head sitting on a tree or something like that. Then you can imagine that that's not straightforward, right? Because you give a text and it should make somehow an image. And one building block within this, let's say, ecosystem from text to image is artificial models. So today we'll also have a look at diffusion models a little bit that you understand. What the basic principles are. We also discuss a little bit more use cases that say why these kind of methods or systems have applications where they have applications, the limitations, things like that. And then we do a starting have a starting deep dive in in rock. Retrieve, augmented generation systems. We will learn what the main components are. You will see how to do it in, I mean, to build it kind of a conversational pipeline with flow wise. So that's an old coding platform. Does somebody know flow wise or ever tried to flow wise? No. Did somebody of you already kind of, you know, codes, retrieval augmented generation based system? No. Okay, so then everything will be new. That's good. So it is no coding platform. You can basically quickly assemble and orchestrate different components, which give you a kind of a chain, for example, from a very, let's say vanilla chat with open AI to something more sophisticated like a retrieval augmented generation system, very neat also vector databases and things like that or other databases. And so we will have a, I will show you how to do that. And then I discuss a little bit with you how you can do it. And the idea would be that next time, at least one part of the lecture will be really hands on that we all work together or that we work in groups for building a system you have in mind or you would like to do. So this is what we, this is what we do today. Okay, I mean the recap. I will go. Quite or yeah, go through when quickly so Mark of chains, Aaron and transformers is what we discussed. And the important the challenge. I did this silly form. I know it's not perfect, but I hope that it helps a little bit to organize the groups. To my best memory, I have in mind that more or less everybody subscribed for challenge one, which is a retrieval augmented. I don't know if there is a kind of a strategy behind this, but yeah, it is like like that for the moment. Nobody asked me for the fifth challenge for the hard one. So that's also okay. Yeah, so the last time we learned about some main components regarding text generation and large language models in general, which means, yeah, transformers at the end. I just want to give you still one remark, probably I did the last time as well, but. I mean, why are transformers so successful? It's they are successful because they work that's the only reason there is no. You know, there is no heart theoretical justification why this kind of systems should work in the first place. Why why they are capable or why you can scale up them in a way that they work even better if you scale up in terms of data and in terms of the size of the of the neural network composed of transformer blocks. So it is just because it works so nicely and there is so far no real alternative on the horizon for that. But it that doesn't mean that at the end some clever people or, you know, a creative student from concern comes up with an idea, which could work better. And it's a little bit that most of the stuff, which is on nowadays is is not really let's say science in a traditional way that you have a hypothesis and then basically you design an experiment, you test the hypothesis. And then the result tells you if you can if the result I mean if if if the experiment.